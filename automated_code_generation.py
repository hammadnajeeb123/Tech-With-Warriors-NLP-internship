# -*- coding: utf-8 -*-
"""automated code generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z90GanBhBub__P8uBC3270NLa9lIQl2y

Install necessary libraries such as transformers, datasets, and torch
"""

!pip install transformers datasets torch

"""Select a transformer model suited for code generation, like GPT-2, CodeGen, or Codex"""

from transformers import AutoModelForCausalLM, AutoTokenizer
model_name = 'gpt2'  # or another model
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

"""Use a dataset for training, like the "CodeSearchNet" dataset, or create your own. Load your dataset using the datasets library"""

from datasets import load_dataset
dataset = load_dataset('code_search_net', 'python')

"""Ensure you have the necessary libraries installed. Run this in a cell"""

!pip install transformers datasets torch

"""Import the required libraries and load the pre-trained transformer model"""

def generate_code(description):
    inputs = tokenizer.encode(description, return_tensors='pt')
    outputs = model.generate(inputs, max_length=500)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

"""Create a function that takes a user prompt in natural language and generates corresponding code"""

from IPython.display import display
import ipywidgets as widgets

input_box = widgets.Text(
    description='Enter your prompt here:',
    placeholder='e.g., Create a function to sort a list',
    layout=widgets.Layout(width='500px')
)

output_box = widgets.Output()
display(input_box, output_box)

"""Use Google Colabâ€™s interactive input feature to allow users to enter their prompts"""

def on_submit(change):
    with output_box:
        output_box.clear_output()
        prompt = input_box.value
        code = generate_code(prompt)
        print("Generated Code:")
        print(code)

input_box.observe(on_submit, names='value')

# Install Required Libraries
!pip install transformers datasets torch ipywidgets

# Import Libraries
from transformers import AutoModelForCausalLM, AutoTokenizer
from IPython.display import display
import ipywidgets as widgets

# Load Pre-trained Model and Tokenizer
model_name = "gpt2"  # You can change this to a different model if needed
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Set pad_token to eos_token if it's not already set
tokenizer.pad_token = tokenizer.eos_token

# Define the Code Generation Function with pad_token_id and attention_mask
def generate_code(description):
    # Encode the user input with attention mask and padding
    inputs = tokenizer(description, return_tensors='pt', padding=True, truncation=True)

    # Generate code from the input with attention mask and pad_token_id
    outputs = model.generate(
        inputs['input_ids'],
        attention_mask=inputs['attention_mask'],
        max_length=150,
        pad_token_id=tokenizer.eos_token_id,  # Set pad_token_id to eos_token_id
        num_return_sequences=1,
        no_repeat_ngram_size=2,  # Prevent repetitive sequences
        early_stopping=True
    )

    # Decode the generated output to text
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Create an Input Interface
input_box = widgets.Text(
    description='Enter your prompt here:',
    placeholder='e.g., Create a function to sort a list',
    layout=widgets.Layout(width='500px')
)

output_box = widgets.Output()
display(input_box, output_box)

# Define a Function to Handle Input and Generate Code
def on_submit(change):
    with output_box:
        output_box.clear_output()  # Clear previous output
        prompt = input_box.value  # Get the user input
        code = generate_code(prompt)  # Generate code from the prompt
        print("Generated Code:")  # Display header
        print(code)  # Print the generated code

# Observe the input box for changes and call on_submit function
input_box.observe(on_submit, names='value')