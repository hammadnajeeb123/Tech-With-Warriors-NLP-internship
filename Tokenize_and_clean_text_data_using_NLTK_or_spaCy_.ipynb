{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMoDQ68jxIXnoPwQKJQi0on",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hammadnajeeb123/TechWithWarrior_NLP-internship/blob/main/Tokenize_and_clean_text_data_using_NLTK_or_spaCy_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup & Import Libraries\n",
        "First, ensure that you have the necessary libraries installed. You can uncomment the installation commands if needed."
      ],
      "metadata": {
        "id": "65tmdUDxUlqi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTKaT8-PT99q",
        "outputId": "a2e0a073-f2c7-4958-cff3-5664ee858994"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.6)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.9.3)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from wordcloud) (10.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from wordcloud) (3.7.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=efacb496a2bfc2d04d3abf4559ba76fe2317ad43d47553187a7aed8c1d58731d\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries (uncomment if needed)\n",
        "!pip install nltk spacy wordcloud langdetect\n",
        "\n",
        "# Download necessary NLTK data (uncomment if needed)\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# For spaCy, download the English language model (uncomment if needed)\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Import necessary libraries\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import string\n",
        "import pickle\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "from spacy.lang.en import English\n",
        "from langdetect import detect, DetectorFactory\n",
        "\n",
        "# Ensures consistent results from langdetect\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "# Initialize NLTK tools\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Define the Enhanced TextPreprocessor Class\n",
        "This class will encapsulate all the preprocessing functionalities."
      ],
      "metadata": {
        "id": "hMrCVKobUrzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextPreprocessor:\n",
        "    def __init__(self, method='nltk', custom_stop_words=None, preserve_punctuation=False):\n",
        "        \"\"\"\n",
        "        Initialize the TextPreprocessor.\n",
        "\n",
        "        :param method: Method to use for preprocessing ('nltk' or 'spacy').\n",
        "        :param custom_stop_words: List of custom stop words to be added.\n",
        "        :param preserve_punctuation: Boolean flag to preserve punctuation.\n",
        "        \"\"\"\n",
        "        self.method = method\n",
        "        self.preserve_punctuation = preserve_punctuation\n",
        "        self.custom_stop_words = custom_stop_words if custom_stop_words else []\n",
        "\n",
        "        if method == 'nltk':\n",
        "            self.lemmatizer = WordNetLemmatizer()\n",
        "            self.stop_words = set(stopwords.words('english'))\n",
        "        elif method == 'spacy':\n",
        "            self.nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "    def preprocess(self, text, apply_ner=False, detect_language=False):\n",
        "        \"\"\"\n",
        "        Preprocess the input text.\n",
        "\n",
        "        :param text: Input text to preprocess.\n",
        "        :param apply_ner: Flag to apply Named Entity Recognition.\n",
        "        :param detect_language: Flag to detect the language of the text.\n",
        "        :return: Processed text and entities (if apply_ner is True).\n",
        "        \"\"\"\n",
        "        if detect_language:\n",
        "            # Only preprocess if text is in English\n",
        "            if detect(text) != 'en':\n",
        "                return text, []\n",
        "\n",
        "        if self.method == 'nltk':\n",
        "            return self._preprocess_nltk(text), []\n",
        "        elif self.method == 'spacy':\n",
        "            return self._preprocess_spacy(text, apply_ner)\n",
        "\n",
        "    def _preprocess_nltk(self, text):\n",
        "        \"\"\"\n",
        "        Preprocess text using NLTK methods.\n",
        "\n",
        "        :param text: Input text to preprocess.\n",
        "        :return: Processed text.\n",
        "        \"\"\"\n",
        "        # Text normalization (convert to lowercase, expand contractions)\n",
        "        text = self._expand_contractions(text.lower())\n",
        "\n",
        "        # Remove special characters and numbers\n",
        "        if not self.preserve_punctuation:\n",
        "            text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "        # Tokenize text into words\n",
        "        tokens = word_tokenize(text)\n",
        "\n",
        "        # Remove stop words and custom stop words\n",
        "        tokens = [word for word in tokens if word not in stopwords.words('english') and word not in self.custom_stop_words]\n",
        "\n",
        "        # Lemmatize tokens\n",
        "        tokens = [self.lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "        # Join tokens back to a single string\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    def _preprocess_spacy(self, text, apply_ner):\n",
        "        \"\"\"\n",
        "        Preprocess text using spaCy methods.\n",
        "\n",
        "        :param text: Input text to preprocess.\n",
        "        :param apply_ner: Flag to apply Named Entity Recognition.\n",
        "        :return: Processed text and entities (if apply_ner is True).\n",
        "        \"\"\"\n",
        "        # Text normalization (convert to lowercase, expand contractions)\n",
        "        text = self._expand_contractions(text.lower())\n",
        "\n",
        "        # Process text with spaCy model\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        # Tokenize, remove stop words, and lemmatize\n",
        "        tokens = [\n",
        "            token.lemma_ for token in doc\n",
        "            if not token.is_stop and token.text not in self.custom_stop_words and (self.preserve_punctuation or token.is_alpha)\n",
        "        ]\n",
        "\n",
        "        # Named Entity Recognition (NER) if enabled\n",
        "        entities = [(ent.text, ent.label_) for ent in doc.ents] if apply_ner else []\n",
        "\n",
        "        # Join tokens back to a single string\n",
        "        return ' '.join(tokens), entities\n",
        "\n",
        "    def _expand_contractions(self, text):\n",
        "        \"\"\"\n",
        "        Expand common contractions in the text.\n",
        "\n",
        "        :param text: Input text.\n",
        "        :return: Text with expanded contractions.\n",
        "        \"\"\"\n",
        "        contractions = {\n",
        "            \"can't\": \"cannot\",\n",
        "            \"won't\": \"will not\",\n",
        "            \"n't\": \" not\",\n",
        "            \"'re\": \" are\",\n",
        "            \"'s\": \" is\",\n",
        "            \"'d\": \" would\",\n",
        "            \"'ll\": \" will\",\n",
        "            \"'t\": \" not\",\n",
        "            \"'ve\": \" have\",\n",
        "            \"'m\": \" am\"\n",
        "        }\n",
        "        pattern = re.compile('|'.join(contractions.keys()))\n",
        "        return pattern.sub(lambda x: contractions[x.group()], text)\n",
        "\n",
        "    def tokenize_sentences(self, text):\n",
        "        \"\"\"\n",
        "        Tokenize the text into sentences.\n",
        "\n",
        "        :param text: Input text to tokenize.\n",
        "        :return: List of sentences.\n",
        "        \"\"\"\n",
        "        if self.method == 'nltk':\n",
        "            return sent_tokenize(text)\n",
        "        elif self.method == 'spacy':\n",
        "            doc = self.nlp(text)\n",
        "            return [sent.text for sent in doc.sents]\n",
        "\n",
        "    def pos_tagging(self, text):\n",
        "        \"\"\"\n",
        "        Perform Part-of-Speech tagging on the text.\n",
        "\n",
        "        :param text: Input text for POS tagging.\n",
        "        :return: List of tokens with their POS tags.\n",
        "        \"\"\"\n",
        "        if self.method == 'nltk':\n",
        "            tokens = word_tokenize(text)\n",
        "            return nltk.pos_tag(tokens)\n",
        "        elif self.method == 'spacy':\n",
        "            doc = self.nlp(text)\n",
        "            return [(token.text, token.pos_) for token in doc]\n",
        "\n",
        "    def generate_word_cloud(self, text, max_words=100):\n",
        "        \"\"\"\n",
        "        Generate a word cloud for visualization.\n",
        "\n",
        "        :param text: Input text for word cloud generation.\n",
        "        :param max_words: Maximum number of words to include in the word cloud.\n",
        "        :return: WordCloud object.\n",
        "        \"\"\"\n",
        "        wordcloud = WordCloud(width=800, height=400, max_words=max_words, background_color='white').generate(text)\n",
        "        return wordcloud\n",
        "\n",
        "    def handle_negations(self, text):\n",
        "        \"\"\"\n",
        "        Handle negations in the text.\n",
        "\n",
        "        :param text: Input text to process for negations.\n",
        "        :return: Text with handled negations.\n",
        "        \"\"\"\n",
        "        tokens = word_tokenize(text)\n",
        "        negation_words = [\"not\", \"n't\", \"no\", \"never\"]\n",
        "        negation_flag = False\n",
        "        processed_tokens = []\n",
        "\n",
        "        for word in tokens:\n",
        "            if word in negation_words:\n",
        "                negation_flag = True\n",
        "            elif negation_flag:\n",
        "                processed_tokens.append(\"NOT_\" + word)\n",
        "                negation_flag = False\n",
        "            else:\n",
        "                processed_tokens.append(word)\n",
        "\n",
        "        return ' '.join(processed_tokens)\n"
      ],
      "metadata": {
        "id": "cd73tpTcUD0G"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Testing the Enhanced Text Preprocessor\n",
        "Now that we have defined the class, we can test it with some sample text to demonstrate its capabilities."
      ],
      "metadata": {
        "id": "Hzcy1vBlU1RT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text for testing\n",
        "sample_text = \"I don't think this is a good idea. I'm not happy with the results. NLTK or spaCy, which one is better?\"\n",
        "\n",
        "# Initialize the TextPreprocessor with custom settings\n",
        "preprocessor = TextPreprocessor(method='spacy', custom_stop_words=['nltk', 'spacy'], preserve_punctuation=True)\n",
        "\n",
        "# Preprocess the text with Named Entity Recognition enabled\n",
        "processed_text, entities = preprocessor.preprocess(sample_text, apply_ner=True, detect_language=True)\n",
        "print(\"Processed Text:\", processed_text)\n",
        "print(\"Named Entities:\", entities)\n",
        "\n",
        "# Sentence tokenization\n",
        "print(\"Sentence Tokenization:\", preprocessor.tokenize_sentences(sample_text))\n",
        "\n",
        "# POS Tagging\n",
        "print(\"POS Tagging:\", preprocessor.pos_tagging(sample_text))\n",
        "\n",
        "# Handling Negations\n",
        "print(\"Text with Negations Handled:\", preprocessor.handle_negations(sample_text))\n",
        "\n",
        "# Word Cloud Visualization (displaying requires a separate environment with graphic support)\n",
        "# Uncomment to visualize in an appropriate environment\n",
        "# wordcloud = preprocessor.generate_word_cloud(processed_text)\n",
        "# wordcloud.to_image().show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T02tujLGUu8C",
        "outputId": "ba0912e5-ee4c-435f-9aac-4601454935e5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Text: think good idea . happy result . , well ?\n",
            "Named Entities: []\n",
            "Sentence Tokenization: [\"I don't think this is a good idea.\", \"I'm not happy with the results.\", 'NLTK or spaCy, which one is better?']\n",
            "POS Tagging: [('I', 'PRON'), ('do', 'AUX'), (\"n't\", 'PART'), ('think', 'VERB'), ('this', 'PRON'), ('is', 'AUX'), ('a', 'DET'), ('good', 'ADJ'), ('idea', 'NOUN'), ('.', 'PUNCT'), ('I', 'PRON'), (\"'m\", 'AUX'), ('not', 'PART'), ('happy', 'ADJ'), ('with', 'ADP'), ('the', 'DET'), ('results', 'NOUN'), ('.', 'PUNCT'), ('NLTK', 'PROPN'), ('or', 'CCONJ'), ('spaCy', 'VERB'), (',', 'PUNCT'), ('which', 'DET'), ('one', 'PRON'), ('is', 'AUX'), ('better', 'ADJ'), ('?', 'PUNCT')]\n",
            "Text with Negations Handled: I do NOT_think this is a good idea . I 'm NOT_happy with the results . NLTK or spaCy , which one is better ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Saving and Loading the Preprocessor\n",
        "You can save the preprocessor object using pickle for later use"
      ],
      "metadata": {
        "id": "is11JifPVAge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the preprocessor object\n",
        "with open('text_preprocessor.pkl', 'wb') as f:\n",
        "    pickle.dump(preprocessor, f)\n",
        "\n",
        "# Load the preprocessor object\n",
        "with open('text_preprocessor.pkl', 'rb') as f:\n",
        "    loaded_preprocessor = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "4vQ2kECDU4Ky"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Uw9_Hq6VDnz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}